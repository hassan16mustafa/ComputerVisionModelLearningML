# -*- coding: utf-8 -*-
"""PytorchCourse Stat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-z3xCVKXIW4dZwwlS2QfA8NtZYKu9ML
"""

print("Hello WOrld!")

!nvidia-smi

"""##00 Pytorch Notebook

Resource Notebook: https://www.learnpytorch.io/00_pytorch_fundamentals/


"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

"""## intro to tensors
## creating tensors

pytorch tensors are creted using torch.tensors
"""

# scalar
scalar = torch.tensor(7)
scalar

scalar.ndim
# GEt tensor back as poython int
scalar.item()

# Vector
vector = torch.tensor([7,7])
vector

# You can figure out dimensions by number pf square brackets
# Vector
vector.ndim

vector.shape
#it means 2 points one line , 2 elements
# shape will be 2 by 1

#MATRIX
MATRIX = torch.tensor([[7,8],
                      [9,10]])
MATRIX

MATRIX.ndim

MATRIX.shape
# shape will be 2 by 2

MATRIX[0]

MATRIX[1]

# Tensor
TENSOR = torch.tensor([[[1,2,3],
 [3,6,9],
  [2,4,5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0][1][1]

TENSOR = torch.tensor([[[[[1,6,9],[3,2,3],[3,2,9],[1,8,7],[5,5,6]]]]])
TENSOR

TENSOR.ndim,TENSOR.shape

"""[link text](https://)### Random Tensors
why random Tensors?
Random Tensors are imp because the way many neural networks learn is that tehy start with tensors full of random numbers  and then adjust those number sto better represent the data.

`Start with random num >- look at data>- updat random num >-look at data >- updat random num


"""

#create a random tensor of size/shape (3,4)
random_tensor = torch.rand(3,4)
random_tensor

random_tensor.ndim

#create a random tensor with similatr shape to image tensor
image_tensor = torch.rand(size=(224,224,3)) # height, width , colour chaneels (R, G, B)
image_tensor.shape,  image_tensor.ndim

randTensor = torch.rand(3,5,5)
randTensor

randTensor.ndim
randTensor.shape

temp = torch.rand(size = (3,3))
temp

torch.rand(3,3)

"""# Zeros and Ones
# tensor of all zeros and all ones  best for masking


"""

# tensor of all zeros
zeros = torch.zeros(3,3)
zeros

zeros + temp
zeros * temp

# all ones tenspor
ones = torch.ones(3,3)
ones

ones.dtype # default datatype

ones + temp

"""### Creating a range of tensor sa nd tensors_like"""

#use torch.range()
torch.arange(0,10)

torch.arange(start=0,end =1000, step =77)

one_to_ten = torch.arange(1,11)
one_to_ten

"""### creating tensors like"""

ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros

tempo = torch.arange(start = 0, end = 1000 , step = 16)
tempo

ones_temp = torch.ones_like(tempo)
ones_temp

"""# tensor data types"""

# Float 32 tensor
float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype = None,# what data type is the tensot (float32, float 16)
                               device = None,# what device your tenspr is pon gpu , cpu
                               requires_grad = False) # whether or not to track gradients

float_32_tensor



randten = torch.tensor([3,6,9])
randten

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)

float_16_tensor * float_32_tensor

float_64_tensor= torch.tensor([3.0,6.0,9.0],
                              dtype = torch.float64)

float_16_tensor * float_64_tensor

int_32_tensor = torch.tensor([3,6,9], dtype = torch.int32)
int_32_tensor

int_32_tensor * float_64_tensor

"""### getting info from tensors ( attriutes)
 Tensores type = tensor.dtype

 tensor not right spage = tensor.shape

 tensor not right device = tensor.device
"""

# create a tensor
tensor = torch.rand(3,4)
tensor

#info will be
print(tensor)
print( tensor.dtype)
print( tensor.shape)
print(tensor.size())
print( tensor.device)

tensor = tensor.type(torch.float64)
tensor

"""### Manipulating tensors ( tensor operations)

Tensor operations include
1. Addition
- Subtraction
- dividon
multiplication
matrrix multiplication



"""

# create atensor
t1 = torch.tensor([1,2,3])
t1 + 20

t1 * 10

t1 /10

t1 -19

# try out built in funcyions
torch.mul(t1,10)

torch.add(t1,100)

"""### Matrix multiplication

Two main ways of performing in neurao networks and deep learning:

ELement-Wise multiplication
Matrix MUltiplication

"""

# elememt wise mutlip
t1* t1

# mtarix multiploication
torch.matmul(t1,t1)

# matrix multi by hansd
(1*1) + (2*2) + (3 *3)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# val = 0
# for i in range(len(t1)):
#   val+= t1[i] + t1[i]
# 
# print(val)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(t1,t1)

"""### One of the most common errors in deep learnong is shap elearning

1rules of mat mul
- Inner dimensions must match
-3,2 @ 3,2 will not work
2,3 @ 3,2 will work
3,2 @ 2,3 will work

_ The resulting matrix has the shape of the outer simensions
2,3  @ 3,2 so it is 2,2
3,2 @ 2,3 so it is 3,3




"""



t1 @ t1

torch.rand(3,2) @ torch.rand(2,3)

torch.rand(10,2) @ torch.rand(2,10)

### SHape Errors in Machine Learning

tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])

tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

tensor_A.shape, tensor_B.shape

"""# to fix our tensor shape issues, we can manipulate the shape of our tensors using a  **transpose

transpose swiches  the dimensions or axes of  agiven tensor
"""

tensor_A.T, tensor_B.T

tensor_B.T , tensor_B.T.shape

(tensor_A @ tensor_B.T).shape

(tensor_A @ tensor_B.T)

(tensor_A.T @ tensor_B)

"""Tensor aggregations , finding mean, max  sum etc"""

# create a tensor
tensor = torch.arange(0,100,10)
tensor

torch.max(tensor), tensor.max()

torch.min(tensor), tensor.min()

tensor = tensor.type(torch.float32)
torch.mean(tensor)
### the torch.mean tensor requires a float dtype
### another way of writing this owuld be tensor.type(torch.float32).mean()

# sum
torch.sum(tensor), tensor.sum()

# finding the positional min and max
torch.argmax(tensor), torch.argmin(tensor), tensor[2], tensor.argmax(), tensor.argmin()

tensor + 1

"""# rehspaing , stacking , squeezing , umsqueezing
-Reshaping - reshapes an input tensor to a defined shape
- view - return a voew of imput tensor of certaion shape but keep the same memory as an originla tensor
- stacking =0- combine multiple tensors over each other , vstack and hstack,

-squeeeze removes a dimension from a tensor

-unsqueeze add a  dimension to a tensor

-permute return a view of the input
"""

x = torch.arange(1., 10.)
x,x.shape

# add extra dimention

x_reshape = x.reshape(1,9)
x_reshape, x_reshape.shape

x_reshape = x.reshape(9,1)
x_reshape, x_reshape.shape

x_reshape = x.reshape(3,3)
x_reshape, x_reshape.shape

x = torch.arange(1., 10.)
x,x.shape

x_reshape = x.reshape(1,9)
x_reshape , x_reshape.size()

# change the view
z = x.view(1,9)
z, z.shape

z = x.view(9,1)
z,z.shape

# chnaging z chnages x , as view shares memory as theoriginla input
z[0] = 5
z,x

# stack some tensors on top of ecahother
x_stack = torch.stack([x,x,x,x], dim = 0)
x_stack

x_stack = torch.stack([x,x,x,x], dim = 1)
x_stack

torch.hstack([x,x,x,x])

torch.vstack([x,x,x,x])

#toch.squeeze() removes all single dimension sfrom  atrget tensor.

 x = torch.arange(1.0,10.0)
 x[0] = 5
 x=x.reshape(1,9)
 x

x.shape

x = x.squeeze()
print(x)

x.shape

x = x.unsqueeze(0) # add a single dim to a tensor at a specific dim
x.shape

x = x.unsqueeze(1)
x,x.shape

# torch.permute returns a view of the orig tensor with the dimensiions chnaged
img = torch.rand(size =(224,224,3))  # hetight , width , colour channeks )
# permute the original tneso rto rearrange the axis or dim order
n_img = img.permute((2,0,1))# shifts axis 0 to 1 , 1 -2 , 2-0
print(f"Previous shape: {img.shape}")
print(f"New shape: {n_img.shape}") # now this si scolor chn]annels height , width
img[0,0,0]= 345
print(n_img[0,0,0])

# tensors  indexing , chnaging values intensors

# indeximg iwth pytorch is similar to numpy

x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

# lets index on our new tensor
x[0] = 4

# torch.squeeze removes all single dimensions from a given tensor
z = torch.arange(1., 10.)
z[0] = 5
z

z.squeeze()#added a dimension  to the tensor

z.squeeze().shape

# Create a tensor 'z' (assuming 'z' already exists)
print(f"Previous tensor: {z}")
print(f"Previous shape: {z.shape}")

# Add an extra dimension by unsqueezing along dimension 0
z = z.unsqueeze(dim=0)

# Print the tensor after unsqueezing
print(f"The unsqueezed tensor: {z}")
print(f"The new shape: {z.shape}")

# indexing in tensors
# xpermuted uses the reference of the original tensor

# indexing with Pytorch is similar to NUmpy
# create a tensor
x = torch.arange(1,10).reshape(1,3,3)
x,x.shape

#lets index on our new tensor
x[0]

x[0,0] # lets index on middle bracket

x[0][0]

# lets index on the most inner bracket
x[0][0][0]

x[0][1][1] # we can only index on the first dimenion as  0

x[0][2][2]

# you can use ':" to select all of a target dimension
x[:,0]

# get all values of 0th and 1st dim but ionly index 1 of 2nd dim
x[:,:,1]

# get all values of 0 dim , but  only the 1 index value of the 1st and 2nd dim
x[:,1,1]# x[0][1][1] , this is same as this but semicolon means aall dim slected so returns [] brcaker

# get index 0 of oth and 1st dim and all values of 2nd dim
x[0,0,:]

# index on x to return 9
x[0][2][2]

x[:,2,2]

#index on x to return to 3,6,9

x[:,:,2]

"""# pytorch tensors and numpy
# numpy is a computing library

* data in numpy, want in pytorch tensor `torch.from_numpy(ndarray)`
* pytorch tenor to numpy `torch.tensor.numpy()


"""

# NUMpy array to tensor
import numpy as np

array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array) # warning numpy to pytorch , it had type float64c of numpy

array,tensor

array.dtype

tensor.dtype

torch.arange(1.0,8.0).dtype

tensor.type(torch.float32)

# change the value of the array
array = array + 1
array,tensor

# tensor to numpy
x = torch.ones(7)
numpy_tensor =x.numpy()
x , numpy_tensor

# change the tenbsor
x = x + 1
x , numpy_tensor
# so

# freproducibility in pytorch

# trying to take the random out of randomm
# in short how a neural networks learns:
# start with random num , tensor operaytions , update nums , again , again , again, again,

# to redice randomnees in neural netowrkds and pytirch comes the concept of seed , essentiallly the random seed " flavour" the randomness

torch.rand(3,3)

# create two random tensors
ra = torch.rand(3,4)
rb = torch.rand(3,4)
ra,rb

print(ra == rb)

# lets makke some random but reproducible tensors
# set the random seed
RANDOM_SEED= 42
torch.manual_seed(RANDOM_SEED)
rc = torch.rand(3,4)
rd = torch.rand(3,4)
rc,rd

RANDOM_SEED= 42
torch.manual_seed(RANDOM_SEED)
rc = torch.rand(3,4)
torch.manual_seed(RANDOM_SEED)
rd = torch.rand(3,4)
rc,rd

print(rc == rd)

"""# differhent ways aof CCESSING  a gpu in pytorch
### Getting a gpu
1. use google collab for free gpu
2. use your own gpu
3. use cloud computing , gcp , azure , aws

"""

### check for gpu access
!nvidia-smi

#setup device agnostic code
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
device

torch.cuda.device_count()

### putiing tensors and models on the gou
# gpu leada to faster computaions
#create a tensor
tensor = torch.tensor([1,2,3], device = "cpu")
print(tensor, tensor.device)

# move tensor to gpu if available

tensor_gpu = tensor.to(device)
tensor_gpu, tensor_gpu.device

###  numpy works on cpu
# tensor_gpu.numpy() this will give an error
# to fix error above
tensor_cpu = tensor_gpu.cpu().numpy()
tensor_cpu, tensor_gpu

"""## GEtting started with python workflow"""

import torch
from torch import nn # conatins  all of oytorchs learning blocks
import matplotlib.pyplot as plt

# check version
torch.__version__

1.# preparing and loading data
# data can be anything in machine learning , images , excel , videos, audio like songs , dna , text#
# machine learning is data in numerical representation
# build patterns in that numerical representation
# to showcase this , ;ets create some known data using the linear regression formual ,
# we will use a linear regresssion formula to make a straight line with knoen parameters

#Create known parameters
weight = 0.7
bias = 0.3
#create a range of numbers
start = 0
end = 1
step =0.02
X = torch.arange(start,end,step).unsqueeze(dim=1)
y = weight * X + bias
X[:10], y[:10],len(X),len(y)

"""### SPlitting data into training and test sets  , most imp for machine learning
Let's create a training and test set for our data
"""

# create a train/test split
train_split = int(0.8 * len(X))
train_split
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train),len(y_train), len(X_test), len(y_test)

X_train , y_train

def plot_predictions(train_data=X_train, train_labels=y_train, test_data=X_test, test_labels=y_test, predictions=None):
    """
    Plots training data, test data, and compares predictions
    """
    plt.figure(figsize=(10, 7))

    # Plot training data in blue
    plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

    # Rest of your code... # plot in green
    plt.scatter(test_data,test_labels,c="g", s=4, label ="Testing data")
    # are there predictions
    if predictions is not None and len(test_data) == len(predictions):
      plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

    # plot the predictions if the

    # show the legenf
    plt.legend(prop={"size": 14});

plot_predictions()

# now we are actually going to build a model
# our first pytorch model

# create a linear regresssion model class

# # everything in pytorch inherits from nn.module
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))
        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.weights * x + self.bias




      # our model strats with random valeues, looks at trainingdra , and adjusr random values to better adjust the ideal values
      # how does it do so
      #1. Through gradient descenr
      #2. Through back propogation of errors

"""from sys import modules
### pytorch model building essentials
1. torch.nn contains all of the building blocks for neural networks
2.torch.nn.parameter what parameters should try and learn ,  often a pytorch layer will set this for us
torch.nn.Module The base class for all neural network modules, if you sub class it , you should override forward
4. torch.optim - this where the optimisers in pytorch live , they willl help in gradient descent
5. def forward() All nn.Module subclasses require you to override forward , thi smethod definses what happens in the forward computation

## Checking the coontenst sof our pytorch model

Now we have created pur model and now lets see whats insode our model
 So we can check out our model parar,eters using `.parameters`
"""

# create a random seed
torch.manual_seed(42)
# create an instance of the model  ( this is a subclass of the nn.module)
model_0 = LinearRegressionModel()

# check out the parameters
list(model_0.parameters())

torch.manual_seed(16)
torch.randn(1)

# list the named parameters
model_0.state_dict()

weight,bias

"""# making predictions with our models torch.inferencemode

To checjk our models precitive power , lets see how well it predicts y_test based on X_test
When we pass data through our model , it goes through the forward method


"""

X_test, y_test

# make predictions with model
with torch.inference_mode():
  y_preds = model_0(X_test)

y_preds

# same thing with torch.nograd()
with torch.no_grad():
  y_preds = model_0(X_test)
y_preds

y_test

plot_predictions(predictions = y_preds)



y_preds = model_0(X_test)
y_preds

"""### Train model
The whoe idea of training is to move from some unknown parameters to some known Parameters
or  go from poor representation to better Representation

One way to measure how poor or how wrong our models predictions are is to use a loss function
NOte: loss function may also be claeed cost func or criterion in differernt areas

Things we need to ttrain:

** loss function:
A function to measure how wromg your models predictions are to the iodeal outpyrsThings we need to ttrain:
1. Optimizer , takes into account the lsoss of a model and adjust a models pararmeters ( weights and bias) to improve the loss function

And for pytorch we need a training lopp and a testing loop


"""

list(model_0.parameters())

# check out pour models parameteres,  a parameter is a value taht our model sets itself
model_0.state_dict()

# aet up a loss function
loss_fn = nn.L1Loss()


# set up an optimizer (schotastic gradient descnet )
optimizer = torch.optim.SGD(params = model_0.parameters(),
                            lr = 0.01)# learning rat e= possibly the most imp learning hyperparameter( the parameter that we set ourselves) # how much change in our parameter

loss_fn

"""# Now we are going to create a training loop
# building a training loop in pytorch and a testing loop
A couple of things we need in a training loop :
1. Loop throw the data
2. Forward pass ( this involves data moving through our models forward funstionbs) also called forward propogation
3. Calculate the loss ( comapre forward pass predictions to ground truth labels)
4. Optimizer zero grad
5.loss backward - moves backwards through the network to calculate the gradients of each o fthe parameters of our model with respect to the loss ( back propogattion)
6. Optimizer step - use the optimizer to adjusr our models nn.parameters to reduce th eloss ( gradient descent)

we need to minimize the gradient ,



"""

list(model_0.parameters())

with torch.inference_mode():
  list(model_0.parameters())

# an epoch is one loop through the data ( this is a hyperparameter because we set it ourselvces)
epochs = 200
epoch_count =[]
loss_values  = []
test_loss_values = []


#### Training
#0. Loop through the data'
for epoch in range(epochs):
  # set the model to training mode
  model_0.train() # train mode in pytorch sets all parameters that require gradients to require gradients

  #1. Forward pass
  y_pred = model_0(X_train)
  #2. Calculate th eloss using the loss func you implemmented
  loss = loss_fn(y_pred, y_train)
  #print(f"Loss: {loss}")

  #3. Optimizer zero grad
  optimizer.zero_grad()



  #4. Perform back propogation on the loss with respect to the parameters of the model
  loss.backward()

  #5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default how th e optimizer changes will accumulate the loop so we have to zero them in step 3 for the next iteration of the loop

  model_0.eval() # turns off gradient tracking and other seetings not needed for testing

  with torch.inference_mode():# turns off gradient tracking and other seetings not needed for testing
     #! . DO the forward pass
     test_pred = model_0(X_test)

     #2.  Calculate th etest loss
     test_loss  = loss_fn(test_pred,y_test)

     if epoch % 10 ==0:
      epoch_count.append(epoch)
      loss_values.append(loss)
      test_loss_values.append(test_loss)

      print(f"Epoch:{epoch} | loss: {loss} loss: Test loss: {test_loss}" )
      print(model_0.state_dict())
      print(" ")











# Print out the model state dict
model_0.state_dict()

"""# my try to rewriting the training loop
its trainng the time,

do the forward pass,

calculate the loss,

optimizer zero gard

losss back wards

optimizer step


### for testing the data
with torch no grda

do the forward pass

calc the loss

 watch ity go down down

# my try to  trgradient to 0
ain the model
1. loop through the model
2. DO the forwrad pass
3. Calculate the error
4. optimizerzero grad
5. back propogation of errords
6. perform th egradient descent
for epoch in range(epochs):
  y_preds = model_0(X_test)

  loss = loss_fn(y_preds, y_train)

  optimizer.zero_grad()
  

  loss.backward()

  optimizer.step()
"""

model_0.state_dict()

# making somme predictions with our model which is not perfedct

with torch.inference_mode():
  y_preds_new  = model_0(X_test)

plot_predictions(predictions=test_pred);

plot_predictions(predictions=y_preds_new)

# plot the loss curves
epoch_count, loss_values, test_loss_values

# we need to coonvert into numpy arrays
import numpy as np
np.array(torch.tensor(loss_values).cpu().numpy()),np.array(torch.tensor(test_loss_values).cpu().numpy())



plt.plot(epoch_count,np.array(torch.tensor(loss_values).cpu().numpy()),label = "Train loss")
plt.plot(epoch_count,np.array(torch.tensor(test_loss_values).cpu().numpy()), label = " Test l;oss")
plt.title(" traing and test loss curves")
plt.ylabel(" loss")
plt.xlabel("EPochs")
plt.legend();

"""## Saving a modelin Pytorch
There are three main methods you should know baout for saving and loading a model in Pytrorch

1. `torch.save()` will save you model in Python picjle format
2. `torch.load()` - allows you load a saved Pytorch object
3.torch.nn.Module.load_state_dict() - this alllows to loads a  models saved state dict

"""

# saving our pytorch model
from pathlib import Path

# create a model directory

MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents = True, exist_ok=True)

# create  a model save path
MODEL_NAME = "01_lINEAR_REGRESSION.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

MODEL_SAVE_PATH # JUST GOT PATH HAVENT SAVED YET

#3 NOE=W SAVE THE MODEL STATE DICT

print(f" SAvionf model to : {MODEL_SAVE_PATH}")

torch.save(obj=model_0.state_dict(),
           f =MODEL_SAVE_PATH)

!ls -l models

"""# loadfing ou rmodel
## loadin a pytorch Model
SInce we saved our models state dict not the entire model , we will create a new instanve  of the model class and load the saved state dict

"""

model_0.state_dict()

# to load in a saved state dict , we ghave to create a new model class
loaded_model_0 = LinearRegressionModel()
# Load the saved state dict of model_0
loaded_model_0.load_state_dict(torch.load(f =MODEL_SAVE_PATH))

loaded_model_0.state_dict()

# now  make some preds with the model

loaded_model_0.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_0(X_test)

loaded_model_preds

# justr for my opwn sake
model_0.eval()
with torch.inference_mode():
  y_pred_my = model_0(X_test)

y_pred_my == loaded_model_preds

# compare loaded model preds wit original
test_pred == loaded_model_preds

"""### putting it all together fpor practoise

"""

#6.1 Data
import torch
from torch import nn
import matplotlib.pyplot as plt

## checking Pytorch version
torch.__version__

"""# create device agnostic code , so if we have gpu it will use it otherwise , cpu by default

#
"""

# set up device agnostic code
device ="cuda" if torch.cuda.is_available() else "cpu"
print(f" usinf device: {device}")

!nvidia-smi

#6.1 Getting some data

# Create some data using the linear regression formula of y = weight * featiures + bias
weight = 0.7
bias = 0.3
# create range values

start = 0
end = 1
step = 0.02
# create x and y ( features and labels)

X= torch.arange(start,end,step).unsqueeze(dim=1)# without unsqueeze errors will pop up
y = weight * X + bias
X[:10], y[:10]

# split the data into test and train
train_split = int(0.8 *len(X))
X_train ,y_train = X[:train_split],y[:train_split]
X_test,y_test = X[train_split:],y[train_split:]
len(X_test),len(X_train)

# plot the data and note if you dont have the plot_predictioons functoion loaded , this will give error
plot_predictions(X_train,y_train,X_test,y_test)

## builkding a pytorch linear model
# create a linear model by subclassing nn.Module

class LinearRegressionModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    # use nn.linear() fro creating the model parameters? also called linear transform , probiong layer , fully connceted layer , dense layer
    self.linear_layer = nn.Linear(in_features=1,
                                   out_features=1)

  def forward(self,x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)

# set the manual seed
torch.manual_seed(42)

model_1 = LinearRegressionModelV2()
model_1,model_1.state_dict()

#Check the model current device
next(model_1.parameters()).device

# set the model to use the target device
model_1.to(device)
next(model_1.parameters()).device

X_train[:5], y_train[:5]

"""###6.3 Training the model

for training need
 we need a loss function and an optimizer and a training loop and a testing loop

"""

loss_fn = nn.L1Loss()

optimizer = torch.optim.SGD(params = model_1.parameters(),
                            lr = 0.01)

# lets write a training loop

torch.manual_seed(42)
epochs = 200


# put data on the gpu , set up device ag cpde for data as well as model
X_train = X_train.to(device)
X_test = X_test.to(device)
y_train = y_train.to(device)
y_test = y_test.to(device)



for epoch in range(epochs):
  model_1.train()
  #  so forward pass

  y_pred = model_1(X_train)

  loss = loss_fn(y_pred,y_train)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  ### testing

  model_1.eval()

  with torch.inference_mode():
    test_pred = model_1(X_test)

    test_loss = loss_fn(test_pred,y_test)

    # print whats happening

    if epoch %10 ==0:
      print(f"epoch is : {epoch} loss: {loss} test loss: {test_loss}")

model_1.state_dict()

# making and evaluating predi tions
model_1.eval()
# make predictions on the test data
with torch.inference_mode():
  y_preds = model_1(X_test)

y_preds

# checking out our model predictions visually
plot_predictions(predictions=y_preds.cpu())

weight, bias

model_1.state_dict()

#6.5 Saving and loading a trained model
from pathlib import Path

#1.Create model directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True,exist_ok =True)
#2. create model save path

MODEL_NAME = "01_MY_TRY_MODEL.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME
MODEL_SAVE_PATH

#.3 SAVE the model state dict
print(f" saving podel to: {MODEL_SAVE_PATH}")


torch.save(obj=model_1.state_dict(),
           f = MODEL_SAVE_PATH)

model_1.state_dict()

# load a pytorch model

# create a new instajce of linear regression model v2
loaded_model_1 = LinearRegressionModelV2()


# load the saved model_1 state dict

loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))

# put the loaded model to device





loaded_model_1.to(device)

next(loaded_model_1.parameters()).device

loaded_model_1.state_dict()

# evaluae ploaded model

loaded_model_1.eval
with torch.inference_mode():
  loaded_model_1_preds = loaded_model_1(X_test)

loaded_model_1_preds

y_preds == loaded_model_1_preds

"""### Neural Network Classification
Classification is aproblem of predicting if somethoing is one thing or another

##1. Make classification data and get it ready
"""

import sklearn

from sklearn.datasets import make_circles
# make 1000 sampels
n_samples = 1000
# create circels
X,y = make_circles(n_samples,
                   noise =0.03,
                   random_state = 42)
len(X),len(y)

print(f"First 5 samples of X:\n {X[:5]}")

print(f"FIrst 5 samples of y:\n {y[:5]}")

# makew data frame of circles
import pandas as pd
circles = pd.DataFrame({"X1": X[:,0],
                        "X2" : X[:,1],
                        "label" : y})


circles.head(10)

import matplotlib.pyplot as plt
plt.scatter(x=X[:,0],
            y=X[:,1],
            c= y,
            cmap = plt.cm.RdYlBu)

"""# note the dat we are working with is often referred to as a toty dataset  but large enough to practise the fundamentalks


we are going to predict if dot is blue or red

1.1 Check input and output shapes
"""

X.shape,y.shape

X

# view the first exampkes of features and labels
# 2 features of x for a label y
X_sample = X[0]
y_sample = y[0]
print(f"values for sample of X: {X_sample} , value for smaple of y: {y_sample}")
print(f"shape for a sample of x {X_sample.shape}, shape for a smapel of y: {y_sample.shape}")

""" turn data into tensors and create test and train splits"""

import torch
torch.__version__

type(X),X.dtype

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)
X[:5],y[:5]

X.dtype,y.dtype

type(X)

# split data into training and test sets

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,
                                                 y,
                                                 test_size = 0.2, # 20 percent will be test and 80 percent will be train
                                                random_state = 42)

len(X_train),len(X_test),len(y_train),len(y_test)

# time = 09:21

"""###2. Building a model to classify outr blue and red dots

to od so, we wan t0:

1. Setup device agnostic code , so our code will run in GPU if there is one
@. Construct a model by subclassing nn.Module
#. Define a loss and a optimizer
Build  a training and tezt loop
"""

import torch
from torch import nn
# Make device agnosto c code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

X_train

"""# Now start constructing a model after setting up device agnostic code

!.subclass nn.module ( amost all modules in pytorch subclass nn.module)
2. create 2 nn.linear() - layers that are capapble of handling the shapes of our data
3. Defibnes a 'forward() ' method that outlines the forward pass
4. Instantatiate an instance of our model class and send it to the target devpice


"""

X_train.shape

# construct a model that subclass nn.Module

class CircleModel(nn.Module):
  def __init__(self):
    super().__init__()
    # 2. Create  2 nn>linear Layer s capabble of handling the shapes of our data
    self.layer_1 =nn.Linear(in_features=2,out_features=5) # takes in 2 features and upscales to 5 features
    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous and return 1 label which is y

  #  self.two_linear_layers = nn.Sequential(
   #     nn.Linear(in_features =2,out_features = 5),
    #    nn.Linear(in_features = 5 , out_features = 1)

    #)

  def forward(self,x):
    return self.layer_2(self.layer_1(x))

  #def forward(self,x):
   # return two_linear_layers(x)




#4. instantantiate a n instrance of our  model and senfd=d it to traget device


model_0 = CircleModel().to(device)
model_0

device, next(model_0.parameters()).device

# lets replicate th emodel abpove usinf nn.Sequential

model_0 = nn.Sequential(
    nn.Linear(in_features =2,out_features = 5),
    nn.Linear(in_features = 5 , out_features = 1)
).to(device)

model_0

model_0.state_dict()

# make some predictions with the model
with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))


print(f" Length of predictions : {len(untrained_preds)} , SHape: {untrained_preds.shape}")
print(f"Length of test samples: {len(X_test)} Shape : {X_test.shape}")
print(f"First 10 predictions : \n{untrained_preds[:10]}")
print(f"FIrst 10 labels : {y_test[:10]}")

X_test[:10],y_test[:10]

"""### set up loss function and optmizer
Which loss function and optimizer shoudl you use

for example regression = predcting an error MAE or MSE ( meean absolut or sqautred error)
for classification , you might want cross entropy or categorical cross entropy


As a remibnder , the loss func measures how wrong your model is

and  fo roptimizers two of the most common and useful are SGD and Adam

For the loss function we are usinbg torch.nn.BCEWithLogitsLoss



"""

# set up the loss function
#loss_fn == nn.BCELoss # requires inputs to have gone through the sigmoid activation]

3#nn.Sequential(
  #  nn.Sigmoid(),
   # nn.BCELoss()
#)


loss_fn = nn.BCEWithLogitsLoss() # this has the sigmoid activation function n=built in

optimizer = torch.optim.SGD(model_0.parameters(),
                            lr =0.1)

# calculate accurcay  - out of 100 examples , what percebt does our model get right?
def accuracy_fn(y_true,y_pred):
  correct = torch.eq(y_true,y_pred).sum().item()
  acc = (correct / len(y_pred)) * 100
  return acc

"""# #. Training the model
we need to build a training loop
the steps are:

forward pass
loss
 x\zero grad
 bcak proppogation
 grdainet desecnt

Going from rae logits >-prediction probbaulities > prediction labels

our model  output sare raw LOGITS

we can cobvert these LOGITS  into production probbailities by passing them into some kind of activation function( e.g sigmoid g=for binary cross entropy , and softmax for multi class classification
                                                                                                                 )

Then we can convert our models are model sprediction posibilties to prediction  labels by eithe rrounding them or taking the argmax
"""

# view the first oyutput sof the forward pass on the test data
model_0.eval()
with torch.inference_mode():
  y_logits = model_0(X_test.to(device))[:5]
y_logits

y_test[:5]

# use the sigmoid activation function our model logit to turn them intgo prediction probbailities
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs

torch.round(y_pred_probs)

"""# for our prediction probbailties values , we need to perform a ranf style rounding on them:

y_pred_probs >= 0.5 and <=1 y= 1 , class = 1
y_pred_probs < 0.5 and >0 y= 0 , class = 0
"""

# finf=d the predicted labels
y_preds = torch.round(y_pred_probs)

# in full'
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

print(torch.eq(y_preds.squeeze(),y_pred_labels.squeeze()))

# get rid of extra dim
y_preds.squeeze()

y_test[:5]

# building a training and test loops
torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 100

# put data to target device

X_train = X_train.to(device)
X_test = X_test.to(device)
y_train = y_train.to(device)
y_test = y_test.to(device)

# build training ans testing loops
for epoch in range(epochs):
  # training the data

  model_0.train()

  #1. forward pass

  y_logit = model_0(X_train).squeeze()

  y_pred = torch.round(torch.sigmoid(y_logit))# turn logit tpo pred probs to pred labels

  # noe calc the loss/ accuracy
  loss = loss_fn(y_logit,
                 y_train) ##nn.BCE with logits loss expects raw logits as input

  #loss = loss_fn(torch.sigmoid(y_logits), y_train)# this expects pred probs as inpout as nn.bce


  acc = accuracy_fn(y_true = y_train,
                 y_pred = y_pred)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  ### testing
  model_0.eval()
  with torch.inference_mode():
    test_logits = model_0(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))


  # calculate th etest loss/acc
  test_loss = loss_fn(test_logits,
                      y_test)

  test_acc = accuracy_fn(y_true = y_test,
                         y_pred = test_pred)

  # print out wghats happen

  if epoch % 10 == 0:
    print(f"Epoch: {epoch} loss: {loss:.5f} Acc: {acc:.2f} test loss: {test_loss:.5f} test acc: {test_acc:.2f}")

circles.label.value_counts()

import torch

# Your model, optimizer, loss function, and accuracy function definitions should be here

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 100

# Transfer data to the target device
X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)

# Training and testing loops
for epoch in range(epochs):
    # Training
    model_0.train()
    y_logit = model_0(X_train).squeeze()
    y_pred = torch.round(torch.sigmoid(y_logit))
    loss = loss_fn(y_logit, y_train)
    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Testing
    model_0.eval()
    with torch.inference_mode():
        test_logits = model_0(X_test).squeeze()
        test_pred = torch.round(torch.sigmoid(test_logits))
        test_loss = loss_fn(test_logits, y_test)
        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)

    if epoch % 10 == 0:
     print(f"Epoch: {epoch} loss: {loss:.5f} Acc: {acc:.2f} test loss: {test_loss:.5f} test acc: {test_acc:.2f}")



"""##4. Make predictions and evaluate the model

FRom the metrics it look slike our model isnt learning anything  , lets ,=make some poredicytoons and make them visual


import a fuc called plot_descison_v=noundary

"""

import requests
from pathlib import Path

# Download helper func from learn pytorch repo (if not already downloaded)

if Path("helper_functions.py").is_file():
    print("It already exists")
else:
    print("Download the helper functions")
    response = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
    with open("helper_functions.py", "wb") as f:
        f.write(response.content)

from helper_functions import plot_predictions, plot_decision_boundary

from helper_functions import plot_predictions, plot_decision_boundary

# plot descion boundary of the model
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_0,X_train,y_train)
plt.subplot(1,2,2)
plt.title("tesr")
plot_decision_boundary(model_0,X_test,y_test)

"""# improving a model from a models perspective
!. add more layers , more chances to learn about patterns in data

add more hidden units - go from 5 hidden unit sto 10 units

FIt for longer, more epochs

changing the activation functions ( other than sigmoid )

chnage the learning rate

Chnage the loss function

stopped at 11:02:40

# let improve by adding more hidden units

add extr alayer

inc number of epochs
"""

import torch.nn as nn

class CircleModel1(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer_1 = nn.Linear(in_features=2, out_features=10)
        self.layer_2 = nn.Linear(in_features=10, out_features=10)
        self.layer_3 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x):
        return self.layer_3(self.layer_2(self.layer_1(x)))


model_1 = CircleModel1().to(device)
model_1

# create a loss function
loss_fn = nn.BCEWithLogitsLoss()

# create an optimizer

optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000


# put data on target device
X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)

for epoch in range(epochs):
  model_1.train()

  y_logits = model_1(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  #2. clavu;ate th eloss and acc'
  loss = loss_fn(y_logits,y_train)
  acc = accuracy_fn(y_true = y_train,
                    y_pred = y_pred)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()


  # testing

  model_1.eval()

  with torch.inference_mode():
    test_logits = model_1(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true = y_test,
                           y_pred = test_pred)

    if epoch % 100 == 0:
     print(f"Epoch: {epoch} loss: {loss:.5f} Acc: {acc:.2f} test loss: {test_loss:.5f} test acc: {test_acc:.2f}")

# plot the decicion boundary
# plot descion boundary of the model
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_1,X_train,y_train)
plt.subplot(1,2,2)
plt.title("tesr")
plot_decision_boundary(model_1,X_test,y_test)

import torch

# Set the parameters
weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.01

# Create data
X_regression = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X_regression + bias
print(len(X_regression))

print(X_regression[:5])
print(y[:5])

#create train and test splits
train_split = int( 0.8 * len(X_regression))
X_train_regression = X_regression[:train_split]
y_train = y[:train_split]
X_test = X_regression[train_split:]
y_test = y[train_split:]
len(X_train_regression), len(y_test)

plot_predictions(train_data = X_train_regression ,train_labels = y_train , test_data=X_test, test_labels=y_test)

# adjusting model_1 to fit a straight line
X_train_regression[0]

# same architecture as model_1
model_2 = nn.Sequential(
    nn.Linear(in_features=1, out_features=10),
    nn.Linear(in_features=10, out_features=10),
    nn.Linear(in_features=10, out_features=1)

  ).to(device)

model_2

loss_fn = nn.L1Loss()

optimizer = torch.optim.SGD(params = model_2.parameters(),
                            lr = 0.01)

# training the model

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000
X_train, X_test, y_train, y_test = X_train_regression.to(device), X_test.to(device), y_train.to(device), y_test.to(device)


for epoch in range(epochs):
  y_pred = model_2(X_train)
  loss = loss_fn(y_pred,y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  # testing
  model_2.eval()
  with torch.inference_mode():
    test_pred = model_2(X_test)
    test_loss = loss_fn(test_pred,y_test)

  # print
  if epoch % 100 == 0:
     print(f"Epoch: {epoch} loss: {loss:.5f}  test loss: {test_loss:.5f}")

#model_2.eval()

#with torch.inference_mode():
 # y_preds = model_2(X_test.cpu())

#plot_predictions(train_data = X_train_regression.cpu() ,train_labels = y_train.cpu() , test_data=X_test.cpu(), test_labels=y_test.cpu(),predictions=y_preds.cpu())

"""1151

##6. The missing piece  of our model NOn linearity

"what patterns coul dyou draw" if you were given an infinite amount of a stright and non straight lines
"""

import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples=n_samples, noise=0.03, random_state=42)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)
plt.show()

import torch
from sklearn.model_selection import train_test_split


# turn data to tensors

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

# split into train and test sets

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)
X_train[:5],y_train[:5]

"""### 6.2 Building model swith  non -linaerity
# BUILD A MODEL WITH NON LINEAR ACTIVATION FUNCTIONS

"""

from torch import nn
class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features = 2, out_features = 10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)
    self.relu = nn.ReLU() # non linear activation function

  def forward(self,x):
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))


model_3 = CircleModelV2().to(device)
model_3

# NEURAL NETS MADE UP OF LINEAR AND NON LINEAR FUNCTIONS TO FIND PATTERNS IN DATA
loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(model_3.parameters(),
                            lr = 0.1)

import torch

# ... (Previous code for data preparation, model definition, etc.) ...

# Training the model
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# Transfer data to the target device
X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)

epochs = 1000
for epoch in range(epochs):
    model_3.train()

    y_logits = model_3(X_train).squeeze()
    y_pred = torch.round(torch.sigmoid(y_logits))
    loss = loss_fn(y_logits, y_train)
    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Testing
    model_3.eval()
    with torch.inference_mode():
        test_logits = model_3(X_test).squeeze()
        test_pred = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)

    if epoch % 100 == 0:
        print(f"Epoch: {epoch} loss: {loss:.5f} Acc: {acc:.2f} test loss: {test_loss:.5f} test acc: {test_acc:.2f}")

### 6.4 Evaluating the model of non linear activation by visualizing
model_3.eval()
with torch.inference_mode():
  y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()

y_preds[:10],y_test[:10]

# plot decision boundaries
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("train")
plot_decision_boundary(model_1,X_train,y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_3,X_test,y_test)

"""## replicatating non linear activation functions

., rather than tellinh what to learn ,w e give tool sto discover patterrns on its own ,

and these tools  are linear and non linear  actibvation functions


"""

# creata a tensor

A = torch.arange(-10,10,1,dtype = torch.float32)
A.dtype

# visualize this data
plt.plot(A)

plt.plot(torch.relu(A))

def relu(x):
  return torch.maximum(torch.tensor(0),x)

relu(A)

# plot relu activation function
plt.plot(relu(A))

def sigmoid(x):
  return 1 / (1+torch.exp(-x))

plt.plot(torch.sigmoid(A))

plt.plot(sigmoid(A))

"""### MMULTI CLASS CLASSIFICATION

## putting it all together with multi-class classification

binatry is cat or dog , spam or not spam

multi class is cat or dog or chicken

# creating a toy multiclass data set
"""

import torch
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# set the hyperpaprameters for dat acreation

NUM_CLASSES = 4
NUM_FEATURES = 2
RANDOM_SEED = 42

#1. Create multiclass data

X_blob,y_blob = make_blobs(n_samples=1000,
                           n_features=NUM_FEATURES,
                           centers = NUM_CLASSES,
                           cluster_std = 1.5,
                           random_state=RANDOM_SEED)

# turn data into temnsors

X_blob = torch.from_numpy(X_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)


X_blob_train , X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,
                                                                         y_blob,
                                                                         test_size = 0.2,
                                                                         random_state = RANDOM_SEED)


# plot the data

plt.figure(figsize=(10,7))
plt.scatter(X_blob[:,0],X_blob[:,1],c=y_blob,cmap = plt.cm.RdYlBu)

### 8.2 building a multi class classificatioon in Pyttorch
#creat edevice agnostoic code
device = "cuda " if torch.cuda.is_available() else "cpu"
device

import torch
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Specify the device

class BlobModel(nn.Module):
    def __init__(self, input_features, output_features, hidden_units=8):
        super().__init__()
        self.linear_layer_stack = nn.Sequential(
            nn.Linear(in_features=input_features, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_features)
        )

    def forward(self, x):
        return self.linear_layer_stack(x)

model_4 = BlobModel(input_features=2, output_features=4).to(device)

print(model_4)

"""#8.3 creare a loss function ans an optimizer"""

loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(params = model_4.parameters(),
                            lr =0.1) # learning rate is a hyper parameter

# getting prediction probbailitie sfor a  multi class pytorch model

# lets get raw outpputs of our model
model_4.eval()
with torch.inference_mode():
  y_logits = model_4(X_blob_test.to(device))

y_preds[:10], y_blob_test[:10]

"""# difference between binary and multi classification

1. one used sigmoid , multi uses softmax
2. binary uses bce , multi ises cross entropy
"""

# convert out=r models logit o/p to prediction probbailties
y_pred_probs= torch.softmax(y_logits,dim=1)
print(y_logits[:5])
print(y_pred_probs[:5])

torch.sum(y_pred_probs[0])

torch.argmax(y_pred_probs[0])

# convert pout=r models prediction probs to labels
y_preds = torch.argmax(y_pred_probs,dim=1)
y_preds

y_blob_test

# logitspred labels (  torch.argmax) { raw model } , pred probs (torch.soft max) >

import torch

# ... (Previous code for data preparation, model definition, etc.) ...

# Training the model
torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000

X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)
X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)

for epoch in range(epochs):
    model_4.train()

    y_logits = model_4(X_blob_train)
    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)

    loss = loss_fn(y_logits, y_blob_train)
    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Testing
    model_4.eval()
    with torch.inference_mode():
        test_logits = model_4(X_blob_test)
        test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)

        test_loss = loss_fn(test_logits, y_blob_test)
        test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_preds)

    if epoch % 100 == 0:
        print(f"Epoch: {epoch} loss: {loss:.5f} Acc: {acc:.2f} test loss: {test_loss:.5f} test acc: {test_acc:.2f}")

# visualize
model_4.eval()
with torch.inference_mode():
  y_logits = model_4(X_blob_test)

y_logits[:10]

y_pred_probs = torch.softmax(y_logits,dim =1)
y_pred_probs[:10]

y_blob_test[:10]

# go from ped probs to lbels
y_preds = torch.argmax(y_pred_probs,dim=1)
y_preds[:10]

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_4,X_blob_train,y_blob_train)
plt.subplot(1,2,2)
plt.title("TEst")
plot_decision_boundary(model_4,X_blob_test,y_blob_test)

"""from os import read
# a few more classification metrics

Accuracy - out of 100 how many our model does get right

Precison

recall

F1- score


classification report
"""

!pip install torchmetrics

"""#Computer Vision and CNN

## Computer Vision libarraie sin pytorch
torchvision = # base domain of computer vision learning libraries
2. torchvision.datasets - get datsets andf data loading models
torchvisoon.models - get pretrained comp viison models
torch.vison.transforms - func for manipulating your vison data to be suitable for use with an ML model
torch.utils.data.Dataset - base data set class for PyTorchFileReader
torch.utils.data.DataLoader- creates python iterable over a dataset
"""

import torch
from torch import nn
import torchvision
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor

import matplotlib.pyplot as plt

torch.__version__,torchvision.__version__

"""## GEtting a dataset
the datset we are taking is FashionMNIST

"""

import torch
from torchvision import datasets, transforms

# Set up the training data
train_data = datasets.FashionMNIST(
    root="data",  # where to download
    train=True,   # get train data
    download=True,
    transform=transforms.ToTensor(),  # how to transform data
)

# Set up the testing data
test_data = datasets.FashionMNIST(
    root="data",   # where to download
    train=False,   # get test data
    download=True,
    transform=transforms.ToTensor(),  # how to transform data
)

len(train_data), len(test_data)

# see the first training example
image,label = train_data[0]
image,label

class_names = train_data.classes
class_names

class_to_idx = train_data.class_to_idx
class_to_idx

train_data.targets

# check the shape of our image
# colour channel ,ht,width
print(image.shape)
print(f" image label :: { class_names[label]}")
print(label)

# one colour channel as grey scale

"""# visualize the image as an imagwe

"""

import matplotlib.pyplot as plt
image.label = train_data[0]
print(f"Image shape : {image.shape}")
plt.imshow(image.squeeze())
plt.title(label)

plt.imshow(image.squeeze(),cmap="gray")
plt.title(class_names[label])
plt.axis(False)

# plot more images
#torch.manual_seed(42)
fig = plt.figure(figsize=(9,9))
rows,cols = 4,4
for i in range(1,rows*cols+1):
  random_idx = torch.randint(0,len(train_data),size = [1]).item()
  print(random_idx)
  img,label = train_data[random_idx]
  fig.add_subplot(rows,cols,i)
  plt.imshow(img.squeeze(),cmap="gray")
  plt.title(class_names[label])
  plt.axis(False)

"""# prepare data loader
# right now our dat ais in the form pytorch data sets
"""

train_data, test_data

"""# data loader converts dat set into python iterable

# we want to turn our data into batches or mini batches
 # we do thi s
 1. it is more efficient
 . so it breaks 60000 images into 32 images at Time
 it gives outr neural net more chances to update its gradients per epoch



"""

from torch.utils.data import DataLoader

# set up the batch size hyperparameter
BATCH_SIZE = 32

# turn datset into iterabkes'=

train_dataloader = DataLoader(dataset = train_data,
                              batch_size = BATCH_SIZE,
                              shuffle = True)


test_dataloader = DataLoader(dataset = test_data,
                              batch_size = BATCH_SIZE,
                              shuffle = False)

test_dataloader,train_dataloader

len(train_dataloader)

len(test_dataloader)

# check whats inside the training data loader
train_features_batch,train_labels_batch = next(iter(train_dataloader))
train_features_batch.shape,train_labels_batch.shape

import torch

# Assuming you have already loaded your train_features_batch and train_labels_batch
# Make sure to replace size[1] with the desired size if it's supposed to be a tuple

#torch.manual_seed(42)
random_idx = torch.randint(0, len(train_features_batch), (1,)).item()
img, label = train_features_batch[random_idx], train_labels_batch[random_idx]
plt.imshow(img.squeeze(),cmap="gray")
plt.axis(False)
plt.title(class_names[label])
print(f"image size: {img.shape}")#
print(f"label: {label}, label size : {label.shape}")

"""# lets a computer vision model

# when building a series f=of ml models , start with base kine model

!. its a simple model that you will try to improve upon with subsequeng models/experiments
in other words , start with basics and add cp=omplexity
"""

# create a flatten layer
flatten_model = nn.Flatten()
x = train_features_batch[0]
# fl;tten the smaple
print(x.shape) #( color channel , ht , wth)
output = flatten_model(x)
print(output.shape) # (color channel =( ht * wth))

output.squeeze()

from torch import nn

class FashionMNISTModelV(nn.Module):
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        super().__init__()

        self.layer_stack = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=input_shape, out_features=hidden_units),
            nn.Linear(in_features=hidden_units, out_features=output_shape)
        )

    def forward(self, x):
        return self.layer_stack(x)

torch.manual_seed(42)
# set up model with input paramerets '

model_0 = FashionMNISTModelV(
    input_shape = 784,
    hidden_units = 10,
    output_shape = len(class_names)
).to("cpu")

model_0

dummy_x = torch.rand([1,1,28,28])
model_0(dummy_x)

"""ting a computer vision model , creating a loss function an optimizer"""

model_0.state_dict()

"""# setting up loss , optimizer and evaluation metrics
#1.loss function will n=be nn.BrossENtrppy loss
#Optimizer will  be SGD
#2.since we are working a classisifi eval metric , we will use acurracy func
#
"""

import requests
from pathlib import Path
 # download hlper fuc from learn pytorch repo
if Path("helper_functions.py").is_file():
  print(" exists")
else:
  print(" doenloaiding")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py","wb") as f:
    f.write(request.content)

# import the accuracy metric
import torch
from helper_functions import accuracy_fn

from torch import nn

# set up loss function

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_0.parameters(),
                             lr=0.1)

accuracy_fn

"""# write a function to time our experimentr

# create  afunction to timme our experiement # machin e learning i svery experiment
# you should track
#1.model performance ( loss and accuracy values
#2. how fast it runs
"""

from timeit import default_timer as timer
def print_train_time(start:float,
                     end:float,
                     device: torch.device= None):
  total_time = end-start
  print(f" train time on {device} total time : {total_time:.3f} seconds")
  return total_time

start_time = timer()
# some code
end_time = timer()
print_train_time(start=start_time,end =end_time, device="cpu")

"""# creating a training lopp and training a model on batches of data


optimizer will update  amoel s performance once per batch rather than epochs

# stwps

1. loop through epochs

2. lopp through training batches e, perform training steps , calculate the train loss per batch

3. loop through testing batches . operform testing stpes , clac the test losss

4. print whats hapeening
"""

# import tqdm for progress bar

from tqdm.auto import tqdm

# set the seed and start the timer
torch.manual_seed(42)
train_time_start_on_cpu = timer()

# set the number of epochs() small for faster training time
epochs = 3

# create training ans test loop
for epoch in tqdm(range(epochs)):
  print(f"EPoch: {epoch}")

  # training loss
  train_loss = 0
  # add a loop to loop through the training batches
  for batch,(X,y) in enumerate(train_dataloader):

    model_0.train()

    # forward pass
    y_pred = model_0(X)

    # calc the loss per batch

    loss = loss_fn(y_pred,y)
    train_loss +=loss #adding up the loss for a batch

    optimizer.zero_grad()

    loss.backward()

    optimizer.step()

    # print out whats happening

    if batch % 400 ==0:
      print(f"looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples")

  # now divid etotal trauin loss by length of train dataloader
  train_loss/= len(train_dataloader)

  # testing
  test_loss, test_acc = 0,0
  model_0.eval()
  with torch.inference_mode():
    for X_test,y_test in test_dataloader:
      # 1. do the forward pass
      test_pred = model_0(X_test)

      # calc loss acc
      test_loss += loss_fn(test_pred,y_test)

      # calc acc
      test_acc += accuracy_fn(y_true = y_test, y_pred = test_pred.argmax(dim =1))

    # calculate the test loss average per bvatch

    test_loss /= len(test_dataloader)

    # calculatet the test acc average per batch
    test_acc /= len(test_dataloader)


  # print whtas happening
  print(f"\nTRain loss: {train_loss:.4f}  test loss {test_loss:.4f}, test acc: {test_acc}")

# calc training time
train_time_end_on_cpu = timer()
total_train_time_model_0 = print_train_time(train_time_start_on_cpu,
                                            end=train_time_end_on_cpu,
                                            device=str(next(model_0.parameters()).device))

next(model_0.parameters()).device

"""## lets evaluate our model
# make predictions and get model_0 results
"""

# evaluating out=r midek
torch.manual_seed(42)
def eval_model(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               accuracy_fn):
  # returns a dic conatining the results of model predictimg on data_loader

  loss,acc= 0,0
  model.eval()
  with torch.inference_mode():
    for X,y in tqdm(data_loader):
      # make our data device ganostic

     # X , y = X.to(device),y.to(device)
      # make predictions
      y_pred = model(X)

      # acc the loss per batcvh
      loss +=loss_fn(y_pred,y)
      acc += accuracy_fn(y_true=y,
                         y_pred = y_pred.argmax(dim=1))

  # scale loss and acc to find avg loss/acc per batch
    loss/=len(data_loader)
    acc/=len(data_loader)
  return{"model_name": model.__class__.__name__,
         "model_loss": loss.item(),
         "model_acc": acc}


# calc model 0 results on test datset


model_0_results = eval_model(model=model_0,
                             data_loader = test_dataloader,
                             loss_fn = loss_fn,
                             accuracy_fn = accuracy_fn)

model_0_results

"""# set up device agnosti ccode
 fore using a gpu if there is one

"""

torch.cuda.is_available()

# set up devce agnostic ocde
import torch
device= "cuda" if torch.cuda.is_available() else "cpu"
device

"""## model 1: BUildingabetter model with non_linearity
# we learned about the power of non linearity in n=boook 2



"""

import torch.nn as nn

class FashionMNISTModelV1(nn.Module):
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        super().__init__()
        self.layer_stack = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=input_shape, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_shape),
            nn.ReLU()  # Added the missing comma before this line
        )

    def forward(self, x: torch.Tensor):  # Corrected the typo here
        return self.layer_stack(x)

# create an instance of model_!
torch.manual_seed(42)
model_1 = FashionMNISTModelV1(input_shape=784,
                              hidden_units = 10,
                              output_shape=len(class_names)).to(device)

next(model_1.parameters()).device

# loss and optim and eval m,etric

from helper_functions import accuracy_fn

loss_fn = nn.CrossEntropyLoss()
optimizer= torch.optim.SGD(params = model_1.parameters(),
                           lr=0.1)



"""# create functions for training and testing loops

"""

def train_step(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               accuracy_fn,  # Type hint could be added if known
               device: torch.device = device):

    # Performs a training with model trying to learn on data_loader
    model.train()

    train_loss, train_acc = 0, 0
    # Loop through the training batches
    for batch, (X, y) in enumerate(data_loader):
        # Put data on the target device
        X, y = X.to(device), y.to(device)

        # Forward pass
        y_pred = model(X)  # Gives us logits

        # Calculate the loss per batch
        loss = loss_fn(y_pred, y)
        train_loss += loss.item()  # Add up the loss for a batch
        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))  # Convert logits to pred labels

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Divide total train loss and accuracy by the length of the train dataloader
    train_loss /= len(data_loader)
    train_acc /= len(data_loader)

    print(f"Train loss = {train_loss:.5f}, Train acc: {train_acc:.2f}")



"""# now creating a testing loop"""

# turning a testing loop into a function
def test_step(model: torch.nn.Module,
              data_loader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              accuracy_fn,
              device:torch.device = device
              ):
  # performs a testing loop step on model going over data_loader
  test_loss, test_acc = 0,0
  # put the data in eval mode
  model.eval()

  with torch.inference_mode():
    for X,y in data_loader:
      X, y = X.to(device), y.to(device)

      #1. DO teh forward pass
      test_pred = model(X)

      #2. Calculate th eloss/ acc
      test_loss += loss_fn(test_pred,y)
      test_acc += accuracy_fn(y_true = y, y_pred = test_pred.argmax(dim=1)) # go from logits to pred labels

    #adjust metrics and print out
    test_loss /= len(data_loader)
    test_acc /= len(data_loader)
    print(f"Test loss = {test_loss:.5f}, Test  acc: {test_acc:.2f}")

# now combing two functions
# measure time

from timeit import default_timer as timer
train_time_start_on_gpu = timer()
# set epochs
epochs = 3
# create optimization and eval loop using train and test step

for epoch in tqdm(range(epochs)):
  print(f"EPoch: {epoch}\n-----")
  train_step(model= model_1,
             data_loader = train_dataloader,
             loss_fn = loss_fn,
             optimizer= optimizer,
             accuracy_fn = accuracy_fn,
             device = device)

  test_step(model= model_1,
            data_loader=test_dataloader,
            loss_fn = loss_fn,
            accuracy_fn = accuracy_fn,
            device = device)

  train_time_end_on_gpu = timer()

  total_train_time_model_1 = print_train_time(start = train_time_start_on_gpu,
                                              end = train_time_end_on_gpu,
                                              device = device)

"""# note sometime sdepnding on data/ hardware sometimes model trains faste r on cpu than  gpu
# it coyld be the overgheard for copying datcapabaility
a / model to and fron the GPU outweighs benefits offered by gpu
# hardwarw you are using a=has a better cpu than gpu in compute
"""



def eval_model(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               accuracy_fn,
               device = device):
  # returns a dic conatining the results of model predictimg on data_loader

  loss,acc= 0,0
  model.eval()
  with torch.inference_mode():
    for X,y in tqdm(data_loader):
      # make our data device ganostic

      X , y = X.to(device),y.to(device)
      # make predictions
      y_pred = model(X)

      # acc the loss per batcvh
      loss +=loss_fn(y_pred,y)
      acc += accuracy_fn(y_true=y,
                         y_pred = y_pred.argmax(dim=1))

  # scale loss and acc to find avg loss/acc per batch
    loss/=len(data_loader)
    acc/=len(data_loader)
  return{"model_name": model.__class__.__name__,
         "model_loss": loss.item(),
         "model_acc": acc}

# get resultys dictionary for model_!
model_1_results = eval_model(model = model_1,
                             data_loader = test_dataloader,
                             loss_fn = loss_fn,
                             accuracy_fn = accuracy_fn,
                             device = device)


model_1_results

model_0_results

from torch import nn
class FashionMNISTModelV2(nn.Module):
  #model architceture that replicates the TINYVGG madel from the CNN explainer website
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        super().__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )

        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )



        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=hidden_units * 7*7, out_features=output_shape)
        )

    def forward(self, x):
        x = self.conv_block_1(x)
        #print(f" output shape of conv blaock 1:{x.shape}")
        x = self.conv_block_2(x)
       # print(f"output shape of conv block  2:{x.shape}")
        x = self.classifier(x)
        #print(f"output shape of classifier {x.shape}")
        return x

image.shape

torch.manual_seed(42)
model_2 = FashionMNISTModelV2(input_shape = 1,
                              hidden_units = 10,
                              output_shape = len(class_names)).to(device)

plt.imshow(image.squeeze(),cmap="gray")

image.shape

# pass the image through model
rand_image_tensor= torch.randn(size =(1,28,28))
rand_image_tensor.shape

model_2(rand_image_tensor.unsqueeze(0).to(device))

"""# stepping through nn.Conv2d
17:45
"""

torch.manual_seed(42)
# create a batch o fimages
images= torch.randn(size= (32,3,64,64))
test_image = images[0]

print(f"image batch shape : {images.shape}")
print(f"single image shappe: {test_image.shape}")
print(f"Test image: \n {test_image}")

#model_2.state_dict()
test_image.shape

# lets create a singl Conv2fd layer
conv_layer = nn.Conv2d(in_channels=3,
                       out_channels = 10,
                       kernel_size = 3,# single number is equivalent of a tuple
                       stride = 1,
                       padding = 1)

# pass the data thrpugh the covolutional layer
conv_output = conv_layer(test_image)
conv_output.shape

# stepping through nn.MaxPool2d
test_image.shape

# print out original shape without unsqueezed dimension
print(f"TEst image of orig hsape : {test_image.shape}")
print(f"test image with unsqueezed dim: {test_image.unsqueeze(0).shape}")

# create a sample of the nn.MaxPool2d later
max_pool_layer = nn.MaxPool2d(kernel_size = 2)

# pass the data through just the conv layer
test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))

print(f"Shape after going through conv layer ; {test_image_through_conv.shape} ")

# pass the data through the max pool layer
test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)
print(f" shape fater going through the two layers : {test_image_through_conv_and_max_pool.shape}")


#

torch.manual_seed(42)
# create  a ranodm tensor with similar number of dimensions to our images
random_tensor = torch.randn(size = (1,1,2,2,))
print(f"random tensor is {random_tensor}")
print(f"random tensor shape : {random_tensor.shape}")
# create a max pool layer
max_pool_layer = nn.MaxPool2d(kernel_size = 2)
# pass the random tensor through the max pool layer
max_pool_tensor = max_pool_layer(random_tensor)
print(f"max pool tensor: {max_pool_tensor}")
print("   ")
print(f"max pool tensor shape: {max_pool_tensor.shape}")

## Training a CNN on out=r own data
from helper_functions import accuracy_fn
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params = model_2.parameters(),lr = 0.1)

##  training and Testing model_2 using our training and test functiojns
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# measure timw
from timeit import default_timer as timer
train_time_start_model_2 = timer()

# train and test model
epochs = 3
for epoch in tqdm(range(epochs)):
  print(f" epoch:{epoch}")
  train_step(model = model_2,
             data_loader = train_dataloader,
             loss_fn = loss_fn,
             optimizer = optimizer,
             accuracy_fn = accuracy_fn,
             device = device)

  test_step(model= model_2,
            data_loader = test_dataloader,
            loss_fn = loss_fn,
            accuracy_fn = accuracy_fn,
            device = device)

  train_time_end_model_2 = timer()
  total_train_time_model_2 = print_train_time(start = train_time_start_model_2,
                                              end=train_time_end_model_2,
                                              device =device)
  # stopeed at 18:34

!nvidia-smi

# get model_2 results
model_2_results = eval_model(
    model = model_2,
    loss_fn = loss_fn,
    data_loader = test_dataloader,
    accuracy_fn = accuracy_fn,
    device=device

)

model_2_results

"""##8. Comapring model results

"""

# create a datfram eto compare results
import pandas as pd
compare_results = pd.DataFrame([model_0_results,
                            model_1_results,
                            model_2_results])

compare_results

# add the training time ofr ll the models
compare_results["Training_time"] = [total_train_time_model_0,
                                    total_train_time_model_1,
                                    total_train_time_model_2]

compare_results

# visualize our model_results
compare_results.set_index("model_name")["model_acc"].plot(kind="barh")
plt.xlabel("accuracy (%)")
plt.ylabel("model")

# make some predictions with random smaple s from the datset
#make and velauate random prediction swith th enest model
def make_predictions(model:torch.nn.Module,
                     data: list,
                     device: torch.device = device):
  pred_probs = []

  model.eval()
  model.to(device)
  with torch.inference_mode():
    for sample in data:
      # prepar ethe sample ( add a batch  dimension and pass to traget device)
      sample = torch.unsqueeze(sample,dim =0 ).to(device)

      # forward pass ( modekl outputs raw logits)
      pred_logit = model(sample)

      # get pred prob
      pred_prob = torch.softmax(pred_logit.squeeze(),dim =0)

      # get pred_prob off gpuu fpr futher calculations
      pred_probs.append(pred_prob.cpu())


# stack th epred_probs to turn list into  atensor

  return torch.stack(pred_probs)

import random
#random.seed(42)
test_samples = []
test_labels = []
for sample,label in random.sample(list(test_data), k=9):
  test_samples.append(sample)
  test_labels.append(label)

# view the first sample shape
test_samples[0].shape

plt.imshow(test_samples[0].squeeze(), cmap="gray")
plt.title(class_names[test_labels[0]])

#  make predictoions

pred_probs = make_predictions(model=model_2,
                              data= test_samples)
pred_probs[:2]

# convert pred probs to labels
pred_classes = pred_probs.argmax(dim=1)
pred_classes

test_labels

# plot predcitions
plt.figure(figsize=(9,9))
nrows = 3
ncols =3
for i, sample in enumerate(test_samples):
  # create subplot
  plt.subplot(nrows,ncols,i+1)

  # plot the target image
  plt.imshow(sample.squeeze(), cmap ="gray")

  # find the prediction in text dorm
  pred_label = class_names[pred_classes[i]]

  # get the truth lable
  truth_label = class_names[test_labels[i]]

# create  atitle fo rthe plot
  title_text = f"Pred: {pred_label}   Truth : {truth_label}"

  # check equality between pred and truth and chnage color of th etitle text

  if pred_label == truth_label:
    plt.title(title_text, fontsize=10, c="g") # green if smae
  else:
    plt.title(title_text, fontsize = 10, c ="r")


  plt.axis(False)

"""# makingf a pconfusion matrix for to further evaluate our model

# make predictiobs with out trained model,
# make a confusion matri x `torch,etricsMAtrix'
# plot the consuioon matrix
"""

import mlxtend
mlxtend.__version__

from tqdm.auto import tqdm

# Make predictions with the trained model
y_preds = []
model_2.eval()
with torch.no_grad():  # Disable gradient computation during inference
    for X, y in tqdm(test_dataloader, desc="Making predictions.."):
        # Send data and targets to the target device
        X, y = X.to(device), y.to(device)

        # Do the forward pass
        y_logit = model_2(X)

        # Turn logits into predicted labels
        y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)

        # Put predictions on CPU for evaluation
        y_preds.append(y_pred.cpu())

# Concatenate the list of predictions into a tensor
#print(y_preds)
y_pred_tensor = torch.cat(y_preds)
y_pred_tensor[:10]

import torchmetrics

try:
  import torchmetrics,mlxtend
  print(f" mlxtend version : {mlxtend.__version__}")
  assert int(mlxtend.__version__.split(".")[1] >=19, "mlxtend version hsould be 19")
except:
  !pip install -q torchmetrics -U mlxtend
  import torchmetrics,mlxtend
  print(f" versoon of mlxtend is {mlxtend.__version__}")

import mlxtend
mlxtend.__version__

from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix

# Set up confusion instance and compare predictions to targets
confmat = ConfusionMatrix(num_classes=len(class_names), task="MULTICLASS")  # Specify the task as "multi_class"
confmat_tensor = confmat(preds=y_pred_tensor,
                         target=test_data.targets)

# Step 3: Plot our confusion matrix
fig, ax = plot_confusion_matrix(
    conf_mat=confmat_tensor.numpy(),  # Matplotlib likes numpy
    class_names=class_names,
    figsize=(10, 7)
)

confmat_tensor

"""# saving loading our best forming label

"""

from pathlib import Path
# craete model directory path
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True,
                   exist_ok = True)

# create model save
MODEL_NAME = " 3RDMODEL.PTH"
MODEL_SAVE_PATH = MODEL_PATH/ MODEL_NAME

# SAVE TH EMODEL STATE DICT

print("FsAVING MODEL TO  {MODEL_SAVE_PATH}")
torch.save(obj=model_2.state_dict(),
           f= MODEL_SAVE_PATH)

model_2.state_dict()

# create a new instance
torch.manual_seed(42)

loaded_model_2 = FashionMNISTModelV2(input_shape = 1,
                                     hidden_units = 10,
                                     output_shape = len(class_names))

# loasd in th esave stat e dict
loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

# send the model to t hetraget device
loaded_model_2.to(device)

# evaluate th eloaded model
model_2_results

torch.manual_seed(42)

loaded_model_2_results = eval_model(
    model= loaded_model_2,
    data_loader = test_dataloader,
    loss_fn = loss_fn,
    accuracy_fn = accuracy_fn
)

loaded_model_2_results

# check if model_reuslts ar eclose to ecahpotehr
torch.isclose(torch.tensor(model_2_results["model_loss"]),
              torch.tensor(loaded_model_2_results["model_loss"]),
              atol =1e-02)

"""# 19:44
#custom Datsets
"""

#Pytorch Custom Datsets
##0. Importing pytorch and setteing up device agnosti cocde
import torch
from torch import nn
print(torch.__version__)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

# get data
# data is subset of food 101 datset
import requests
import zipfile
from pathlib import Path
data_path = Path("data/")
image_path = data_path / "pizza_steak_sushi"


if image_path.is_dir():
  print(f"{image_path}")
else:
  print(f"{image_path}")
  image_path.mkdir(parents=True,exist_ok=True)

# download
#pizza steak and susj=hi data
with open(data_path/"pizza_steak_sushi.zip","wb") as f:
  request = requests.get("https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip")
  print("DOwnlaoding the data")
  f.write(request.content)

# unzip
with zipfile.ZipFile(data_path/ "pizza_steak_sushi.zip","r") as zip_ref:
  print("unzipping")
  zip_ref.extractall(image_path)

"""# becoming one with data or data prep or converting data to tensors"""

import os
def walk_through_dir(dir_path):
  for dirpath,dirnames,filenames in os.walk(dir_path):
    print(f"ther eare {len(dirnames)} directories and  {len(filenames)} images and in '{dirpath}'.")

walk_through_dir(image_path)

# set up training and test path
train_dir = image_path / "train"
test_dir = image_path / "test"

"""# visualizing and image
# get all the image paths
 # pick a range image path
 # get the image clas anme
 # Since we are workimg with images , open the images
 # we will tehn show th eimage and print metadata
"""

import random
from PIL import Image
torch.manual_seed(42)
# get all the image paths
image_path_list = list(image_path.glob("*/*/*.jpg")) # we want evry image path

# pick a random image patyh\

random_image_path = random.choice(image_path_list)
print(random_image_path)


#3. get the image class from path name
image_class = random_image_path.parent.stem
print(image_class)

# open image
img = Image.open(random_image_path)

# print meta dta
print(f"Random image path: { random_image_path}  image class : { image_class}| image height {img.height} | image width; {img.width}")
img

#image_path_list

"""# visulaizing image with matplot lib"""

import numpy as np
import matplotlib.pyplot as plt
# tuen the image into an array
img_as_array = np.asarray(img)

# plot the imag ewith matplotlib
plt.figure(figsize=(10,7))
plt.imshow(img_as_array)
plt.title(f"image class:m {image_class} | iamge shape {img_as_array.shape}") # height , wifdth , colour channels
plt.axis(False)

img_as_array

"""# transforming all dat ainto tensors

Before we can use our image data wit PyTorchFileReader
turn your data into tensors ( in our case , nunmerical representation of our images).

turn into a tirch.utils.data.datset anmd then into dataloader
"""

import torch
from torch.utils.data import DataLoader
from torchvision import datasets,transforms

# write a transfrom for image
data_transform = transforms.Compose([
    transforms.Resize(size =(64,64)), #
    transforms.RandomHorizontalFlip(p=0.5), # randomly flip the iamge
    # turn dat ainto tensor
    transforms.ToTensor()
]

)

data_transform(img), data_transform(img).shape, data_transform(img).dtype

def plot_transformed_images(image_paths: list,transform,n=3,seed=None):
  # selects random image from apth o fimage sand transformsn thema nd plots the orig and transformed versuion

  if seed:
    random.seed(seed)
  random_image_paths = random.sample(image_paths, k=n)
  for image_path in random_image_paths:
    with Image.open(image_path) as f:
      fig,ax = plt.subplots(nrows=1,ncols=2)
      ax[0].imshow(f)
      ax[0].set_title(f"original size: {f.size}")

      #transform and plot the target =image shape
      transformed_image = transform(f).permute(1,2,0)
      ax[1].imshow(transformed_image)
      ax[1].set_title(f"Transformed shape: {transformed_image.shape}")
      ax[1].axis("off")

      fig.suptitle(f"Class: {image_path.parent.stem}", fontsize=16)

plot_transformed_images(image_paths = image_path_list,
                              transform = data_transform,
                              n=3,
                              seed=42


      )

"""# 21 hours

## 4. Option 1 : Loading imag edata using imag efolder
We can load imagfe classifictaiond dat  ausing torchvisions.datsets.ImageFolder
"""



# loading by imagefolder
from torchvision import datasets
train_data = datasets.ImageFolder(root=train_dir,
                                 transform = data_transform, # transform for data
                                 target_transform=None) # transform for label

test_data = datasets.ImageFolder(root=test_dir,
                                 transform = data_transform # transform for data
) # transform for label

test_data,train_data

train_dir,test_dir

# get class names as a list
class_names = train_data.classes

class_names

# get class name as a dict
class_dict = train_data.class_to_idx

class_dict

# check th elen o fdataset
len(train_data), len(test_data)

train_data.samples[0]

# index on the train dat dataset to get a single imag eand label
img,label=train_data[0][0], train_data[0][1]
img,label
print(f"image tensor : {img}")
print(f"image shape {img.shape}")
print(f"image dtype {img.dtype}")
print(f"image label {label}")
print(f"image label dtype {type(label)}")

class_names[label]

# reaarang ethe order of dimensions
img_permute =img.permute(1,2,0)
print(f"original shape : {img.shape}")
print(f"image permute shape : {img_permute.shape}")


# plot the image
plt.figure(figsize=(10,7))
plt.imshow(img_permute)
plt.axis("off")
plt.title(class_names[label])

"""#turn you rdatasets into dataloaders

A dataloader i sgou=ing to help us to turn our datsets into oterable sand custom our batchsize , so data can see batchsize image at a time

"""

# turn out=r train and test datsets into data loaders
from torch.utils.data import DataLoader
train_dataloader= DataLoader(dataset=train_data,
                             batch_size =1,
                             num_workers=os.cpu_count(),
                             shuffle = True)

test_dataloader  = DataLoader(dataset=test_data,
                             batch_size =1,
                             num_workers=1,
                             shuffle = False)

import os
os.cpu_count()

train_dataloader,test_dataloader

len(train_dataloader),len(test_dataloader)

len(train_data), len(test_data)

img,label = next(iter(train_dataloader))

# batch size will be 1
print(f"image shpee {img.shape}") # wull add batch size , colour channel , ht , width

print(f"label shpee {label.shape}")

"""# creating a custom dataset class incase ImageFolder dne

# loading image data with a custom dat aset
# 1. we want to able load images from filr
# we want class anmes from datset
# we we want to be to get dic from datset
"""

import os
import pathlib
import torch

from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms
from typing import Tuple,Dict, List

"""# 5.1 Creating a helper function to get class anmes
we want function to
1. get the class name s using os.scandir to traverse a target directory and data needs to be in stadard image classification format

2. Raise an error if class names arent fminbound
3. turm our class neams into adict or list and return them
"""

#set up traget directory
target_directory = train_dir
print(f"Target dir: {target_directory}")

# get the class names target directory
class_names_found = sorted([entry.name for entry in list(os.scandir(target_directory))])
class_names_found

list(os.scandir(target_directory))

def find_classes(directory:str) -> Tuple[List[str], Dict[str,int]]:
  # finds class folder names in a target dorectory
  #1. get the class name sby scanning target directory
  classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())
  if not classes:
    raise FileNotFoundError(f"COuldnt find class in {directory}")

  # create a dict of index labela
  class_to_idx = {class_name: i for i, class_name in enumerate(classes)}

  return classes, class_to_idx

find_classes(target_directory)

"""# create a custom datse to rpelicate Image Folder

# to create our own cusy=tom datset

we weant to subclass torch.utils.data.datset

iniit our subclass with a target  and transform it if we wane

Create several attributes:

Paths the paths of our images

transform the transformto use

classes - a lisr of tyarget calsses

class_to_idx = a dict o ftarget classes mappe dto inetger labels

We want to create a func to load images

we wanr to overwrite the len method

We want yo override the get item method , to retuen a given smaple when passes an index
"""

from torch.utils.data import Dataset
import pathlib
from PIL import Image
import torch
from typing import Tuple

class ImageFolderCustom(Dataset):
    def __init__(self, tar_dir: str, transform=None):
        self.tar_dir = pathlib.Path(tar_dir)
        self.paths = list(self.tar_dir.glob("*/*.jpg"))
        self.transform = transform
        self.classes = sorted([d.name for d in self.tar_dir.glob('*')])

        # Create a class_to_idx mapping
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}

    def load_image(self, index: int) -> Image.Image:
        # Opens an image via a path and returns it
        image_path = self.paths[index]
        return Image.open(image_path)

    def __len__(self) -> int:
        return len(self.paths)

    # Overwrite __getitem()__ method to return a particular sample
    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:
        img = self.load_image(index)
        class_name = self.paths[index].parent.name  # This expects path in format: data_folder/class_name/.jpeg
        class_idx = self.class_to_idx[class_name]

        if self.transform:
            return self.transform(img), class_idx  # Return transformed image and data, label (X, y)
        else:
            return img, class_idx

from torchvision import transforms

train_transforms = transforms.Compose([
    transforms.Resize(size=(64, 64)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor()
])

test_transforms = transforms.Compose([
    transforms.Resize(size=(64, 64)),
    transforms.ToTensor()
])

# tes t oyt image folde rcustom
train_data_custom = ImageFolderCustom(tar_dir =train_dir,
                                      transform= train_transforms)

test_data_custom = ImageFolderCustom(tar_dir =test_dir,
                                      transform= test_transforms)

train_data_custom,test_data_custom

len(train_data), len(train_data_custom)

len(test_data), len(test_data_custom)

train_data_custom.class_to_idx

# checl fror equality between imag efolder nad custpo m image folder
print(train_data_custom.classes == train_data.classes)
print(test_data_custom.classes == test_data.classes)

"""##5.3 CReate a functio to display random images

1. Take in a dataset and a nuumber of the rpara metsers sucgh as class names and how many image sto c=visualize

2. lets cap the numbe ro fimage sto see at 10

Set the random seed for reproducibilty


get a  list of random sample indexes from the target datset

Setup a matplot lib plot_confusion_matrix

loop tyhrough the random smaple images an dplot hem with matplotlib
make sure dim match with matplot lin
"""

import random
import matplotlib.pyplot as plt
import torch
from typing import List

def display_random_images(dataset: torch.utils.data.Dataset,
                          classes: List[str] = None,
                          n: int = 10,  # Set a default value for n
                          display_shape: bool = True,
                          seed: int = None):
    if n > 10:
        n = 10
        display_shape = False

    if seed:
        random.seed(seed)

    # get random sample indexes
    random_samples_idx = random.sample(range(len(dataset)), k=n)

    # set up matplotlib
    plt.figure(figsize=(16, 8))

    # loop through random indexes and plot them with matplotlib
    for i, targ_sample in enumerate(random_samples_idx):
        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]

        # adjust tensor dimensions for plotting
        targ_image_adjust = targ_image.permute(1, 2, 0)

        # plot adjusted samples
        plt.subplot(1, n, i + 1)
        plt.imshow(targ_image_adjust)
        plt.axis("off")

        title = ""
        if classes:
            title = f"Class: {classes[targ_label]}"
            if display_shape:
                title += f"\nShape: {targ_image_adjust.shape}"

        plt.title(title)

# Example usage:
# display_random_images(your_dataset, classes=your_class_names, n=5, display_shape=True, seed=42)

random.sample(range(len(train_data_custom)), k=10)

# display random images
display_random_images (train_data,
                       n=5,
                       classes= class_names,
                       seed=None)

# display random images from the image oflder custom datset
display_random_images(train_data_custom,
                      n=5,
                      classes= class_names,
                      seed=42)

"""## turn custom loaded images intp dataloders"""

from torch.utils.data import DataLoader
train_dataloader_custom = DataLoader(dataset= train_data_custom,
                                     batch_size =32,
                                     num_workers=0,
                                     shuffle = True)

test_dataloader_custom = DataLoader(dataset= test_data_custom,
                                     batch_size =32,
                                     num_workers=0,
                                     shuffle = False)

train_dataloader_custom, test_dataloader_custom

# Get imag eand label from custom dataloader

img_custom,label_custom = next(iter(train_dataloader_custom))

img_custom.shape, label_custom.shape

"""## other forsm of transformimg data
# data Augmentatiojn
"""

from torchvision import transforms

# First transform with TrivialAugmentWide
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Note the corrected syntax here
    transforms.TrivialAugmentWide(num_magnitude_bins=31),
    transforms.ToTensor()
])

# Second transform without TrivialAugmentWide
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Note the corrected syntax here
    transforms.ToTensor()
])

# get all ikmage paths
image_path_list = list(image_path.glob("*/*/*.jpg"))
image_path_list[:10]

plot_transformed_images(
    image_paths = image_path_list,
    transform = train_transform,
    n=3,
    seed=None
)

"""## building a computer vision odel TINYVGG without dat aaugmentation

### creating some transformsa ad loading data for modle 0
"""

# create a simpl etransform

simple_transform = transforms.Compose([
    transforms.Resize(size=(64,64)),
    transforms.ToTensor()

])

from torchvision import datasets, transforms
import os

# Define your transformation (simple_transform) first
simple_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

# Load and transform the training data
train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)

# Load and transform the test data
test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)

# Turn datasets into dataloaders
train_dataloader_simple = torch.utils.data.DataLoader(dataset=train_data_simple, batch_size=32, shuffle=True, num_workers=os.cpu_count())
test_dataloader_simple = torch.utils.data.DataLoader(dataset=test_data_simple, batch_size=32, shuffle=True, num_workers=os.cpu_count())

"""### create tiny VGG model class

"""

import torch
import torch.nn as nn

class TinyVGG(nn.Module):
    "model architecture from cnn explainer"
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        super().__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,
                         stride=2)
        )

        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units,
                      out_channels=hidden_units,
                      kernel_size=3,
                      stride=1,
                      padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,
                         stride=2)
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=hidden_units*13*13,
                      out_features=output_shape)
        )

    def forward(self, x):
        x = self.conv_block_1(x)
        #print(x.shape)
        x = self.conv_block_2(x)
       # print(x.shape)
        x = self.classifier(x)
       # print(x.shape)
        return x

torch.manual_seed(42)
model_0 = TinyVGG(input_shape = 3,
                  hidden_units =10,
                  output_shape = len(class_names)).to(device)

model_0

### try aforward pass on a single image
# get a single image
image_batch, label_batch = next(iter(train_dataloader_simple))
image_batch.shape, label_batch.shape

# try a forward pass
model_0(image_batch.to(device))

"""## torch info to get an idea o f shaping going through pour model"""

try:
  import torchinfo
except:
  !pip install torchinfo
  import torchinfo

from torchinfo import summary
summary(model_0, input_size =[1,3,64,64])

"""# gtraining and test loops

train step - takes model and data loaded_model_2_results test step takes model and data loaer and evaluates it

"""

import torch

def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               device=device):
    model.train()  # Set the model to training mode
    train_loss, train_acc = 0, 0

    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()  # Clear gradients

        y_pred = model(X)

        loss = loss_fn(y_pred, y)
        train_loss += loss.item()

        loss.backward()

        optimizer.step()

        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
        train_acc += (y_pred_class == y).sum().item() / len(y_pred)

    train_loss = train_loss / len(dataloader)
    train_acc = train_acc / len(dataloader)

    return train_loss, train_acc

import torch

def test_step(model: torch.nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              device=device):
    model.eval()
    test_loss, test_acc = 0, 0

    with torch.no_grad():  # Disable gradient computation during inference
        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            test_pred_logits = model(X)

            loss = loss_fn(test_pred_logits, y)
            test_loss += loss.item()

            test_pred_labels = test_pred_logits.argmax(dim=1)

            test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)

        test_loss = test_loss / len(dataloader)
        test_acc = test_acc / len(dataloader)

    return test_loss, test_acc

from tqdm.auto import tqdm

def train(model: torch.nn.Module,
          train_dataloader: torch.utils.data.DataLoader,  # Corrected dataLoader to DataLoader
          test_dataloader: torch.utils.data.DataLoader,  # Corrected dataLoader to DataLoader
          optimizer: torch.optim.Optimizer,  # Corrected Optimizer to Optimizer
          loss_fn=torch.nn.CrossEntropyLoss(),  # Corrected nn.Module == nn.CrossEntropyLoss() to nn.CrossEntropyLoss()
          epochs: int = 5,
          device=device):

    results = {"train_loss": [],
               "train_acc": [],
               "test_loss": [],
               "test_acc": []}

    for epoch in tqdm(range(epochs)):  # Corrected epocj to epoch
        train_loss, train_acc = train_step(model=model,
                                           dataloader=train_dataloader,  # Corrected data_loader to dataloader
                                           loss_fn=loss_fn,
                                           optimizer=optimizer,
                                           device=device)
        test_loss, test_acc = test_step(model=model,
                                        dataloader=test_dataloader,  # Corrected data_loader to dataloader
                                        loss_fn=loss_fn,
                                        device=device)

        print(f"epoch: {epoch} | train_loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_loss {test_loss:.4f} | test_acc {test_acc:.4f}")

        results["train_loss"].append(train_loss)  # Corrected train_losss to train_loss
        results["train_acc"].append(train_acc)
        results["test_loss"].append(test_loss)
        results["test_acc"].append(test_acc)

    return results

torch.manual_seed(42)
torch.cuda.manual_seed(42)

NUM_EPOCHS =5

model_0 = TinyVGG(input_shape=3,
                  hidden_units =10,
                  output_shape = len(train_data.classes)).to(device)

loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(params= model_0.parameters(),
                             lr=0.01)

# start the timer
from timeit import default_timer as timer
start_time = timer()

# train modle 0

model_0_results = train(model = model_0,
                        train_dataloader = train_dataloader_simple,
                        test_dataloader=test_dataloader_simple,
                        optimizer = optimizer,
                        loss_fn = loss_fn,
                        epochs = NUM_EPOCHS)


end_time = timer()

print(f"total train time : {end_time - start_time:.3f} seconds")

# plot the loss curve
model_0_results.keys()

def plot_loss_curves(results: Dict[str,List[float]]):
  loss = results["train_loss"]
  test_loss = results["test_loss"]

  accuracy = results["train_acc"]
  test_accuracy = results["test_loss"]

  epochs = range(len(results["train_loss"]))

  plt.figure(figsize=(15,7))

  # set up
  plt.subplot(1,2,1)

  plt.plot(epochs, loss , label ="train_loss")
  plt.plot(epochs, test_loss , label ="test_loss")
  plt.title("Loss")
  plt.xlabel("Epochss")
  plt.legend()


  # plot the accuracy

  plt.subplot(1,2,2)
  plt.plot(epochs, accuracy , label = "accuracy")
  plt.plot(epochs, test_accuracy , label = "testaccuracy")
  plt.title("Accuracy")
  plt.xlabel("epochss")
  plt.legend()

plot_loss_curves(model_0_results)

"""# model 1 with data augmentation

"""

# create training transform with tribial augmentat

from torchvision import transforms
train_transforms_trivial = transforms.Compose([
    transforms.Resize(size=(64,64)),
    transforms.TrivialAugmentWide(num_magnitude_bins=31),
    transforms.ToTensor()
])

test_transforms_simple = transforms.Compose([
    transforms.Resize(size=(64,64)),
    transforms.ToTensor()
])

# turn image filders into datasets

from torchvision import datasets
train_data_augmented = datasets.ImageFolder(root = train_dir, transform = train_transforms_trivial)
test_data_simple = datasets.ImageFolder(root = test_dir, transform = test_transforms_simple)

import os
import torch

# Set a random seed
torch.manual_seed(42)

# Create data loaders for the augmented training dataset and the test dataset
train_dataloader_augmented = torch.utils.data.DataLoader(dataset=train_data_augmented,
                                                        batch_size=32,
                                                        shuffle=True,
                                                        num_workers=os.cpu_count())

test_dataloader_augmented = torch.utils.data.DataLoader(dataset=test_data_simple,
                                                       batch_size=32,
                                                       shuffle=False,
                                                       num_workers=os.cpu_count())

"""# construct and train model 1
# we rae changing the dat ahere


"""

torch.manual_seed(42)

model_1 = TinyVGG(input_shape = 3,
                  hidden_units =10,
                  output_shape = len(class_names)).to(device)

torch.manual_seed(42)
torch.cuda.manual_seed(42)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params = model_1.parameters(),
                             lr = 0.01)

from timeit import default_timer as timer
start_timer = timer()

model_1_results = train(model= model_1,
                        train_dataloader = train_dataloader_augmented,
                        test_dataloader = test_dataloader_simple,
                        loss_fn = loss_fn,
                        optimizer = optimizer,
                        epochs = 5,
                        device = device)

end_time = timer()

print(f" total  training time = {end_time - start_timer:.3f}")

model_1_results

plot_loss_curves(model_1_results)

"""# compare model reuslts]]

"""

import pandas as pd
model_0_df = pd.DataFrame(model_0_results)
model_1_df = pd.DataFrame(model_1_results)
model_1_df

plt.figure(figsize=(15,10))

epochs = range(len(model_0_df))

# plot train loss
plt.subplot(2,2,1)
plt.plot(epochs , model_0_df["train_loss"], label =" model 0")
plt.plot(epochs , model_1_df["train_loss"], label =" model 1")
plt.title("Train loss")
plt.xlabel("epochs")
plt.legend();


plt.subplot(2,2,2)
plt.plot(epochs , model_0_df["test_loss"], label =" model 0")
plt.plot(epochs , model_1_df["test_loss"], label =" model 1")
plt.title("Test loss")
plt.xlabel("epochs")
plt.legend();


# plot train loss
plt.subplot(2,2,3)
plt.plot(epochs , model_0_df["acc"], label =" model 0")
plt.plot(epochs , model_1_df["acc"], label =" model 1")
plt.title("acc")
plt.xlabel("epochs")
plt.legend();


plt.subplot(2,2,4)
plt.plot(epochs , model_0_df["test_acc"], label =" model 0")
plt.plot(epochs , model_1_df["test_acc"], label =" model 1")
plt.title("Test acc")
plt.xlabel("epochs")
plt.legend();

import requests
from pathlib import Path

custom_image_path = data_path / "04-pizza-dad.jpeg"

if not custom_image_path.is_file():
    with open(custom_image_path, "wb") as f:
        request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg")
        print(f"downloading {custom_image_path}")
        f.write(request.content)
else:
    print("already downloaded")

"""convert image to tensor or data into which model wa strained on"""

import torchvision
# read in custom image

custom_image_unit8 = torchvision.io.read_image(str(custom_image_path))

plt.imshow(custom_image_unit8.permute(1,2,0))

custom_image_unit8.shape , custom_image_unit8.dtype

custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32) / 255

custom_image

plt.imshow(custom_image.permute(1,2,0))

# create ttrasnfrom pieline to resize

custom_image_trasnform = transforms.Compose([
    transforms.Resize(size=(64,64))
])

"""# making a predictoion on custom fata"""

custom_image = custom_image_trasnform(custom_image)
print(custom_image.shape)

plt.imshow(custom_image.permute(1,2,0))

custom_image = custom_image.unsqueeze(dim=0)
custom_image.shape

model_1.eval()
with torch.inference_mode():
  custom_image_pred = model_1(custom_image.to(device))

custom_image_pred

class_names

# convert logits to pred probs
custom_image_pred_probs  = torch.softmax(custom_image_pred, dim = 1)

custom_image_pred_probs

custom_image_pred_labels = torch.argmax(custom_image_pred_probs, dim =1).cpu()
custom_image_pred_labels

class_names[custom_image_pred_labels]

"""# fuc weher we passa a file path , model pred and gives output"""

import torch
import torchvision
import matplotlib.pyplot as plt
from typing import List

def pred_plot_image(model: torch.nn.Module,
                    image_path: str,
                    class_names: List[str] = None,
                    transform=None,
                    device=device):
    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)
    target_image /= 255

    if transform:
        target_image = transform(target_image)

    model.to(device)
    model.eval()

    with torch.inference_mode():
        target_image = target_image.unsqueeze(0)

    target_image_pred_probs = model(target_image)

    # Convert pred probs
    target_image_pred_labels = torch.argmax(target_image_pred_probs, dim=1)

    plt.imshow(target_image.squeeze().permute(1, 2, 0))

    if class_names:
        title = f"Pred: {class_names[target_image_pred_labels]}"
    else:
        title = f"Pred: {target_image_pred_labels.item()}"

    plt.title(title)
    plt.axis(False)
    plt.show()

pred_plot_image(model= model_1,
                image_path = custom_image_path,
                class_names = class_names,
                transform = custom_image_trasnform,
                device = device)





